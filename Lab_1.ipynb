{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El 2805 Lab 1\n",
    "By: David Rommedahl, Ning Wang\n",
    "\n",
    "## Problem 1a:\n",
    "\n",
    "To model the problem as an MDP we can consider the maze.py code that was used for Lab 0, modifying it to accomodate for the case of the added minotaur.\n",
    "After modifying the actual maze so that it fits the new description we have to define the state space, action space, transition probabilities and the rewards.\n",
    "\n",
    "\n",
    "#### State Space\n",
    "We consider the state space:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{S} = \\left\\{(i,j,m,n): \\textrm{such that maze}[i,j]\\neq 1 \\textrm{ and the cell }(m,n) \\textrm{ is any cell within the maze}\\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "This way the indexes $(i,j)$ represent the player's position within the maze and $(m,n)$ represent the minotaur's position, and any combination of these positions within the maze that do not put the player inside of a wall are valid states. The size of the state space is greatly increased from the model of Lab 0, but the model is conceptually simple.\n",
    "\n",
    "#### Action Space\n",
    "The action space is the same as for Lab 0:\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "\n",
    "#### Transition Probabilities\n",
    "The transition probabilities is the part of this lab where the greatest modification had to be made to the model from Lab 0. The movements of the player are deterministic (if the player chooses to move left the player will move left with probability 1 as long as moving left is a legal move. If it is not a legal move the player will remain in the same position), but the minotaur performs a random walk with the only limitation being that it is not allowed to stay in the same place (it is not allowed to perform a $\\textit{Snake}$-esque move of moving out of the maze on one side and reappearing on the other side of the maze either). As each state is defined as the position of the player $\\textit{and}$ the position of the minotaur, the state transition probabilities will be random with a probability given by the possible next positions for the minotaur.\n",
    "\n",
    "For example, if the player makes a legal move to the right and the minotaur is in a position where it is allowed to move in all four directions there are four possible next states for the action $\\textit{move right}$, each with a transition probability of $\\frac{1}{4}$.\n",
    "\n",
    "   - If at state $s$ action $a$ does not lead to a wall the transition probability is $\\mathbb{P}(s'\\mid s, a)=\\frac{1}{s_p}$, where $s_p$ is the number of possible next positions for the minotaur.\n",
    "   - If at state $s$ action $a$ $\\textit{does}$ lead to a wall the transition probability is $\\mathbb{P}(s\\mid s, a)=1$. This means that both the player and minotaur will stay in the same position. The fact that the minotaur does not move in this case does not matter for the implementation.\n",
    "   - If the player has reached the end state or is eaten by the minotaur ($(i,j)=(m,n)$) the transition probability is $\\mathbb{P}(s \\mid s, a) = 1$.\n",
    "\n",
    "#### Rewards\n",
    "\n",
    "The objective is to find the exit of the maze while avoiding obstacles and not being eaten by the minotaur:\n",
    "\n",
    "   - If the player walks into an obstacle or is eaten by the minotaur the player receives a reward of $r(s,a)=-100$\n",
    "   - If the player reaches the end state they receive a reward of $r(s,a)=0$\n",
    "   - If the player makes a legal move they receive a reward of $r(s,a)=-1$\n",
    "\n",
    "Since the transition probabilities are random in this implementation the rewards are received after an action is taken, which is consistent with the course litterature.\n",
    "\n",
    "#### Problem 1b:\n",
    "\n",
    "In problem 1b we are expected to compute a policy which maximizes the probability of exiting the maze using dynamic programming and to illustrate this policy. We have decided to do this by using the methods for simulating and plotting a scenario already implemented for Lab 0, with modifications made to suite our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze as mz\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "# Modified maze from lab0. Now corresponding to maze in lab1\n",
    "\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "env = mz.Maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dynamic Programming\n",
    "\n",
    "# Finite horizon\n",
    "horizon = 20\n",
    "method = \"DynProg\"\n",
    "\n",
    "# Solve the MDP problem with dynamic programming\n",
    "V, policy = mz.dynamic_programming(env, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simulate the shortest path starting from position A\n",
    "start = (0, 0, 6, 5)\n",
    "path = env.simulate(start, policy, method)\n",
    "\n",
    "# Show the shortest path\n",
    "mz.animate_solution(maze, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1c:\n",
    "\n",
    "The next problem is to vary the time horizon, $T = 1,\\dots,30$. For each $T$, a policy which maximizes the probability of exiting the maze should be found, and the probability should be plotted. Finally, the case when the minotaur is allowed to stand still should also be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.10000000e+03 -2.00000000e+03 -1.90000000e+03 ... -3.00000000e+02\n",
      "  -2.00000000e+02 -1.00000000e+02]\n",
      " [-1.50000000e+01 -1.50000000e+01 -1.50000000e+01 ... -3.00000000e+00\n",
      "  -2.00000000e+00 -1.00000000e+00]\n",
      " [-1.51307332e+01 -1.51307332e+01 -1.51307332e+01 ... -3.00000000e+00\n",
      "  -2.00000000e+00 -1.00000000e+00]\n",
      " ...\n",
      " [-3.00000000e+00 -3.00000000e+00 -3.00000000e+00 ... -3.00000000e+00\n",
      "  -2.00000000e+00 -1.00000000e+00]\n",
      " [-2.00000000e+00 -2.00000000e+00 -2.00000000e+00 ... -2.00000000e+00\n",
      "  -2.00000000e+00 -1.00000000e+00]\n",
      " [-2.10000000e+03 -2.00000000e+03 -1.90000000e+03 ... -3.00000000e+02\n",
      "  -2.00000000e+02 -1.00000000e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_probability = np.empty(30)\n",
    "\n",
    "for horizon in range(1, 31):\n",
    "    V, policy = dynamic_programming(env, horizon)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
